# Phase 4: CI Integration and Automation

## Overview

**Goal:** Automate benchmark execution, regression detection, and performance reporting in the CI/CD pipeline.

**Duration:** 4-6 hours

**Outcome:** Continuous performance monitoring with automated regression detection, historical tracking, and actionable alerts for performance degradation.

---

## Prerequisites

- [x] Phase 3 completed (all benchmarks implemented)
- [x] Baseline performance documented
- [x] Performance profile complete
- [x] Local benchmark execution verified

---

## Phase Objectives

1. Create GitHub Actions workflow for benchmark execution
2. Implement regression detection logic
3. Generate automated performance reports
4. Store and track historical results
5. Configure alerting for performance degradation
6. Document CI/CD integration
7. Verify end-to-end automation

---

## Step 4.1: GitHub Actions Workflow (2 hours)

### Task: Create `.github/workflows/benchmarks.yml`

Automated benchmark execution on PRs and main branch:

```yaml
name: Performance Benchmarks

on:
  pull_request:
    branches: [main, develop]
    paths:
      - 'panama-wrapper/src/**'
      - 'src/jabcode/**'
      - '.github/workflows/benchmarks.yml'
  push:
    branches: [main]
  schedule:
    # Run weekly benchmarks for trending
    - cron: '0 2 * * 0'  # Sunday 2 AM UTC
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter (e.g., EncodingBenchmark.*)'
        required: false
        default: '.*'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours for full suite
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for comparison
      
      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'
          cache: 'maven'
      
      - name: Cache benchmark results
        uses: actions/cache@v3
        with:
          path: panama-wrapper/benchmark-history
          key: benchmark-results-${{ github.run_id }}
          restore-keys: |
            benchmark-results-
      
      - name: Build native library
        run: |
          cd src/jabcode
          mkdir -p build
          cd build
          cmake ..
          make
          cd ../../..
          mkdir -p lib
          cp src/jabcode/build/libjabcode.so lib/
      
      - name: Build Java wrapper
        run: |
          cd panama-wrapper
          mvn clean install -DskipTests
      
      - name: Run benchmarks
        id: benchmarks
        run: |
          cd panama-wrapper
          mkdir -p benchmark-results
          
          LD_LIBRARY_PATH=../lib mvn exec:exec \
            -Dexec.executable="java" \
            -Dexec.args="-cp %classpath --enable-native-access=ALL-UNNAMED \
                         -Djava.library.path=../lib \
                         org.openjdk.jmh.Main ${{ github.event.inputs.benchmark_filter || '.*' }} \
                         -rf json -rff benchmark-results/current.json \
                         -wi 3 -i 5 -f 1"  # Reduced iterations for CI
        continue-on-error: true
      
      - name: Detect regressions
        id: regression
        run: |
          cd panama-wrapper
          java -cp target/test-classes \
            com.jabcode.panama.benchmarks.RegressionDetector \
            benchmark-history/baseline.json \
            benchmark-results/current.json \
            --threshold 10 \
            --output regression-report.md
      
      - name: Generate performance report
        run: |
          cd panama-wrapper
          java -cp target/test-classes \
            com.jabcode.panama.benchmarks.BenchmarkResultsAnalysis \
            benchmark-results/current.json > performance-report.md
      
      - name: Store results in history
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          cd panama-wrapper
          mkdir -p benchmark-history
          cp benchmark-results/current.json \
             "benchmark-history/$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA::7}.json"
          
          # Keep only last 30 runs
          ls -t benchmark-history/*.json | tail -n +31 | xargs -r rm
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: |
            panama-wrapper/benchmark-results/
            panama-wrapper/performance-report.md
            panama-wrapper/regression-report.md
          retention-days: 90
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('panama-wrapper/performance-report.md', 'utf8');
            const regression = fs.readFileSync('panama-wrapper/regression-report.md', 'utf8');
            
            const body = `## üìä Performance Benchmark Results\n\n${report}\n\n## üîç Regression Analysis\n\n${regression}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
      
      - name: Fail on regression
        if: steps.regression.outputs.regression_detected == 'true'
        run: |
          echo "::error::Performance regression detected!"
          exit 1

  benchmark-comparison:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Download current results
        uses: actions/download-artifact@v3
        with:
          name: benchmark-results
          path: current
      
      - name: Download baseline results
        uses: dawidd6/action-download-artifact@v2
        with:
          workflow: benchmarks.yml
          branch: main
          name: benchmark-results
          path: baseline
        continue-on-error: true
      
      - name: Generate comparison report
        run: |
          if [ -f baseline/current.json ]; then
            python3 scripts/compare-benchmarks.py \
              baseline/current.json \
              current/current.json \
              --output comparison.md
          else
            echo "No baseline found, skipping comparison"
          fi
      
      - name: Upload comparison
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-comparison
          path: comparison.md
```

---

## Step 4.2: Regression Detection Implementation (1.5 hours)

### Task: Create RegressionDetector.java

Automated regression detection with configurable thresholds:

```java
package com.jabcode.panama.benchmarks;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.io.File;
import java.io.FileWriter;
import java.util.*;

/**
 * Detects performance regressions by comparing current results to baseline.
 * Used in CI pipeline to fail builds on significant performance degradation.
 */
public class RegressionDetector {
    
    private static final double DEFAULT_THRESHOLD = 10.0; // 10% slowdown
    
    private final ObjectMapper mapper = new ObjectMapper();
    private final double thresholdPercent;
    
    public RegressionDetector(double thresholdPercent) {
        this.thresholdPercent = thresholdPercent;
    }
    
    /**
     * Compare current results to baseline and detect regressions
     */
    public RegressionReport detectRegressions(String baselinePath, String currentPath) 
            throws Exception {
        
        List<BenchmarkResultsAnalysis.BenchmarkResult> baseline = 
            loadResults(baselinePath);
        List<BenchmarkResultsAnalysis.BenchmarkResult> current = 
            loadResults(currentPath);
        
        List<Regression> regressions = new ArrayList<>();
        List<Improvement> improvements = new ArrayList<>();
        
        for (BenchmarkResultsAnalysis.BenchmarkResult cur : current) {
            Optional<BenchmarkResultsAnalysis.BenchmarkResult> base = 
                findMatch(baseline, cur);
            
            if (base.isPresent()) {
                double change = calculateChange(base.get(), cur);
                
                if (change > thresholdPercent) {
                    regressions.add(new Regression(
                        cur.benchmark(),
                        cur.params(),
                        base.get().score(),
                        cur.score(),
                        change
                    ));
                } else if (change < -5.0) { // 5% faster = improvement
                    improvements.add(new Improvement(
                        cur.benchmark(),
                        cur.params(),
                        base.get().score(),
                        cur.score(),
                        -change
                    ));
                }
            }
        }
        
        return new RegressionReport(regressions, improvements, thresholdPercent);
    }
    
    /**
     * Calculate percentage change (positive = slower, negative = faster)
     */
    private double calculateChange(BenchmarkResultsAnalysis.BenchmarkResult baseline,
                                     BenchmarkResultsAnalysis.BenchmarkResult current) {
        return ((current.score() - baseline.score()) / baseline.score()) * 100.0;
    }
    
    /**
     * Find matching baseline result
     */
    private Optional<BenchmarkResultsAnalysis.BenchmarkResult> findMatch(
            List<BenchmarkResultsAnalysis.BenchmarkResult> baseline,
            BenchmarkResultsAnalysis.BenchmarkResult current) {
        
        return baseline.stream()
            .filter(b -> b.benchmark().equals(current.benchmark()))
            .filter(b -> paramsMatch(b.params(), current.params()))
            .findFirst();
    }
    
    private boolean paramsMatch(Map<String, String> p1, Map<String, String> p2) {
        if (p1.size() != p2.size()) return false;
        return p1.entrySet().stream()
            .allMatch(e -> Objects.equals(p2.get(e.getKey()), e.getValue()));
    }
    
    private List<BenchmarkResultsAnalysis.BenchmarkResult> loadResults(String path) 
            throws Exception {
        JsonNode root = mapper.readTree(new File(path));
        List<BenchmarkResultsAnalysis.BenchmarkResult> results = new ArrayList<>();
        
        for (JsonNode node : root) {
            results.add(new BenchmarkResultsAnalysis.BenchmarkResult(
                node.get("benchmark").asText(),
                extractParams(node.get("params")),
                node.get("primaryMetric").get("score").asDouble(),
                node.get("primaryMetric").get("scoreError").asDouble(),
                node.get("primaryMetric").get("scoreUnit").asText()
            ));
        }
        
        return results;
    }
    
    private Map<String, String> extractParams(JsonNode params) {
        Map<String, String> result = new HashMap<>();
        params.fields().forEachRemaining(e -> result.put(e.getKey(), e.getValue().asText()));
        return result;
    }
    
    /**
     * Main entry point for CI usage
     */
    public static void main(String[] args) throws Exception {
        if (args.length < 2) {
            System.err.println("Usage: RegressionDetector <baseline.json> <current.json> [--threshold N] [--output file.md]");
            System.exit(1);
        }
        
        String baselinePath = args[0];
        String currentPath = args[1];
        double threshold = DEFAULT_THRESHOLD;
        String outputPath = null;
        
        // Parse optional arguments
        for (int i = 2; i < args.length; i++) {
            if ("--threshold".equals(args[i]) && i + 1 < args.length) {
                threshold = Double.parseDouble(args[++i]);
            } else if ("--output".equals(args[i]) && i + 1 < args.length) {
                outputPath = args[++i];
            }
        }
        
        RegressionDetector detector = new RegressionDetector(threshold);
        RegressionReport report = detector.detectRegressions(baselinePath, currentPath);
        
        String markdown = report.toMarkdown();
        
        if (outputPath != null) {
            try (FileWriter writer = new FileWriter(outputPath)) {
                writer.write(markdown);
            }
        } else {
            System.out.println(markdown);
        }
        
        // Set GitHub Actions output
        if (System.getenv("GITHUB_OUTPUT") != null) {
            try (FileWriter writer = new FileWriter(System.getenv("GITHUB_OUTPUT"), true)) {
                writer.write("regression_detected=" + !report.regressions().isEmpty() + "\n");
            }
        }
        
        // Exit with error if regressions found
        System.exit(report.regressions().isEmpty() ? 0 : 1);
    }
    
    public record Regression(
        String benchmark,
        Map<String, String> params,
        double baselineScore,
        double currentScore,
        double degradationPercent
    ) {}
    
    public record Improvement(
        String benchmark,
        Map<String, String> params,
        double baselineScore,
        double currentScore,
        double improvementPercent
    ) {}
    
    public record RegressionReport(
        List<Regression> regressions,
        List<Improvement> improvements,
        double thresholdPercent
    ) {
        
        public String toMarkdown() {
            StringBuilder sb = new StringBuilder();
            
            if (regressions.isEmpty()) {
                sb.append("‚úÖ **No performance regressions detected**\n\n");
                sb.append(String.format("All benchmarks within %.1f%% threshold.\n\n", thresholdPercent));
            } else {
                sb.append("‚ö†Ô∏è **Performance Regressions Detected**\n\n");
                sb.append(String.format("Found %d benchmark(s) slower than %.1f%% threshold:\n\n",
                    regressions.size(), thresholdPercent));
                
                sb.append("| Benchmark | Baseline | Current | Change |\n");
                sb.append("|-----------|----------|---------|--------|\n");
                
                for (Regression r : regressions) {
                    sb.append(String.format("| %s | %.2f ms | %.2f ms | **+%.1f%%** üî¥ |\n",
                        formatBenchmarkName(r.benchmark(), r.params()),
                        r.baselineScore(),
                        r.currentScore(),
                        r.degradationPercent()));
                }
                sb.append("\n");
            }
            
            if (!improvements.isEmpty()) {
                sb.append("üìà **Performance Improvements**\n\n");
                sb.append(String.format("Found %d benchmark(s) with notable improvements:\n\n",
                    improvements.size()));
                
                sb.append("| Benchmark | Baseline | Current | Change |\n");
                sb.append("|-----------|----------|---------|--------|\n");
                
                for (Improvement i : improvements) {
                    sb.append(String.format("| %s | %.2f ms | %.2f ms | **%.1f%%** ‚úÖ |\n",
                        formatBenchmarkName(i.benchmark(), i.params()),
                        i.baselineScore(),
                        i.currentScore(),
                        i.improvementPercent()));
                }
                sb.append("\n");
            }
            
            return sb.toString();
        }
        
        private String formatBenchmarkName(String benchmark, Map<String, String> params) {
            if (params.isEmpty()) {
                return benchmark;
            }
            
            String paramStr = params.entrySet().stream()
                .map(e -> e.getKey() + "=" + e.getValue())
                .reduce((a, b) -> a + ", " + b)
                .orElse("");
            
            return String.format("%s (%s)", benchmark, paramStr);
        }
    }
}
```

### Task: Add RegressionDetector Tests

```java
package com.jabcode.panama.benchmarks;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.io.TempDir;

import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Map;

import static org.junit.jupiter.api.Assertions.*;

@DisplayName("RegressionDetector Tests")
class RegressionDetectorTest {
    
    @Test
    @DisplayName("Should detect regression when slower than threshold")
    void testRegressionDetection(@TempDir Path tempDir) throws Exception {
        var detector = new RegressionDetector(10.0);
        
        // Create baseline: 100ms
        String baseline = """
            [
              {
                "benchmark": "TestBenchmark.test",
                "params": {},
                "primaryMetric": {"score": 100.0, "scoreError": 5.0, "scoreUnit": "ms/op"}
              }
            ]
            """;
        
        // Create current: 120ms (20% slower - regression!)
        String current = """
            [
              {
                "benchmark": "TestBenchmark.test",
                "params": {},
                "primaryMetric": {"score": 120.0, "scoreError": 5.0, "scoreUnit": "ms/op"}
              }
            ]
            """;
        
        Path baselinePath = tempDir.resolve("baseline.json");
        Path currentPath = tempDir.resolve("current.json");
        Files.writeString(baselinePath, baseline);
        Files.writeString(currentPath, current);
        
        var report = detector.detectRegressions(baselinePath.toString(), currentPath.toString());
        
        assertEquals(1, report.regressions().size());
        assertEquals(20.0, report.regressions().get(0).degradationPercent(), 0.01);
    }
    
    @Test
    @DisplayName("Should not detect regression within threshold")
    void testNoRegressionWithinThreshold(@TempDir Path tempDir) throws Exception {
        var detector = new RegressionDetector(10.0);
        
        String baseline = """
            [{"benchmark": "Test", "params": {},
              "primaryMetric": {"score": 100.0, "scoreError": 5.0, "scoreUnit": "ms/op"}}]
            """;
        
        // 5% slower - within 10% threshold
        String current = """
            [{"benchmark": "Test", "params": {},
              "primaryMetric": {"score": 105.0, "scoreError": 5.0, "scoreUnit": "ms/op"}}]
            """;
        
        Path baselinePath = tempDir.resolve("baseline.json");
        Path currentPath = tempDir.resolve("current.json");
        Files.writeString(baselinePath, baseline);
        Files.writeString(currentPath, current);
        
        var report = detector.detectRegressions(baselinePath.toString(), currentPath.toString());
        
        assertTrue(report.regressions().isEmpty());
    }
    
    @Test
    @DisplayName("Should detect improvements")
    void testImprovementDetection(@TempDir Path tempDir) throws Exception {
        var detector = new RegressionDetector(10.0);
        
        String baseline = """
            [{"benchmark": "Test", "params": {},
              "primaryMetric": {"score": 100.0, "scoreError": 5.0, "scoreUnit": "ms/op"}}]
            """;
        
        // 10% faster - improvement!
        String current = """
            [{"benchmark": "Test", "params": {},
              "primaryMetric": {"score": 90.0, "scoreError": 5.0, "scoreUnit": "ms/op"}}]
            """;
        
        Path baselinePath = tempDir.resolve("baseline.json");
        Path currentPath = tempDir.resolve("current.json");
        Files.writeString(baselinePath, baseline);
        Files.writeString(currentPath, current);
        
        var report = detector.detectRegressions(baselinePath.toString(), currentPath.toString());
        
        assertEquals(1, report.improvements().size());
        assertEquals(10.0, report.improvements().get(0).improvementPercent(), 0.01);
    }
    
    @Test
    @DisplayName("Should generate markdown report")
    void testMarkdownGeneration() {
        var regression = new RegressionDetector.Regression(
            "SlowBenchmark", Map.of(), 100.0, 120.0, 20.0
        );
        var improvement = new RegressionDetector.Improvement(
            "FastBenchmark", Map.of(), 100.0, 80.0, 20.0
        );
        
        var report = new RegressionDetector.RegressionReport(
            java.util.List.of(regression),
            java.util.List.of(improvement),
            10.0
        );
        
        String markdown = report.toMarkdown();
        
        assertTrue(markdown.contains("Performance Regressions"));
        assertTrue(markdown.contains("SlowBenchmark"));
        assertTrue(markdown.contains("+20.0%"));
        assertTrue(markdown.contains("Performance Improvements"));
        assertTrue(markdown.contains("FastBenchmark"));
    }
}
```

---

## Step 4.3: Historical Tracking (1 hour)

### Task: Create BenchmarkHistory.java

Track and analyze performance trends over time:

```java
package com.jabcode.panama.benchmarks;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.io.File;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.stream.Collectors;

/**
 * Tracks benchmark results over time and analyzes trends
 */
public class BenchmarkHistory {
    
    private final ObjectMapper mapper = new ObjectMapper();
    private final DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyyMMdd-HHmmss");
    
    /**
     * Load all historical results from directory
     */
    public List<HistoricalResult> loadHistory(String historyDir) throws Exception {
        File dir = new File(historyDir);
        if (!dir.exists() || !dir.isDirectory()) {
            return List.of();
        }
        
        List<HistoricalResult> history = new ArrayList<>();
        File[] files = dir.listFiles((d, name) -> name.endsWith(".json"));
        
        if (files != null) {
            for (File file : files) {
                String filename = file.getName();
                LocalDateTime timestamp = parseTimestamp(filename);
                String commitHash = parseCommitHash(filename);
                
                List<BenchmarkResultsAnalysis.BenchmarkResult> results = loadResults(file);
                history.add(new HistoricalResult(timestamp, commitHash, results));
            }
        }
        
        return history.stream()
            .sorted(Comparator.comparing(HistoricalResult::timestamp))
            .collect(Collectors.toList());
    }
    
    /**
     * Analyze trend for specific benchmark
     */
    public Trend analyzeTrend(List<HistoricalResult> history, 
                               String benchmarkName,
                               Map<String, String> params) {
        
        List<DataPoint> points = history.stream()
            .flatMap(hr -> hr.results().stream()
                .filter(r -> r.benchmark().equals(benchmarkName))
                .filter(r -> paramsMatch(r.params(), params))
                .map(r -> new DataPoint(hr.timestamp(), r.score())))
            .collect(Collectors.toList());
        
        if (points.size() < 2) {
            return new Trend(benchmarkName, params, points, TrendDirection.INSUFFICIENT_DATA, 0.0);
        }
        
        // Calculate linear regression
        double slope = calculateSlope(points);
        double avgChange = (slope / points.get(0).value()) * 100.0; // % change per run
        
        TrendDirection direction;
        if (Math.abs(avgChange) < 1.0) {
            direction = TrendDirection.STABLE;
        } else if (avgChange > 0) {
            direction = TrendDirection.DEGRADING;
        } else {
            direction = TrendDirection.IMPROVING;
        }
        
        return new Trend(benchmarkName, params, points, direction, avgChange);
    }
    
    /**
     * Calculate slope using linear regression
     */
    private double calculateSlope(List<DataPoint> points) {
        int n = points.size();
        double sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
        
        for (int i = 0; i < n; i++) {
            double x = i; // Time index
            double y = points.get(i).value();
            sumX += x;
            sumY += y;
            sumXY += x * y;
            sumX2 += x * x;
        }
        
        return (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
    }
    
    private LocalDateTime parseTimestamp(String filename) {
        // Format: 20260112-143045-abc1234.json
        String timestamp = filename.substring(0, 15);
        return LocalDateTime.parse(timestamp, formatter);
    }
    
    private String parseCommitHash(String filename) {
        // Extract hash between last dash and .json
        int start = filename.lastIndexOf('-') + 1;
        int end = filename.lastIndexOf('.');
        return filename.substring(start, end);
    }
    
    private boolean paramsMatch(Map<String, String> p1, Map<String, String> p2) {
        if (p1.size() != p2.size()) return false;
        return p1.entrySet().stream()
            .allMatch(e -> Objects.equals(p2.get(e.getKey()), e.getValue()));
    }
    
    private List<BenchmarkResultsAnalysis.BenchmarkResult> loadResults(File file) throws Exception {
        JsonNode root = mapper.readTree(file);
        List<BenchmarkResultsAnalysis.BenchmarkResult> results = new ArrayList<>();
        
        for (JsonNode node : root) {
            results.add(new BenchmarkResultsAnalysis.BenchmarkResult(
                node.get("benchmark").asText(),
                extractParams(node.get("params")),
                node.get("primaryMetric").get("score").asDouble(),
                node.get("primaryMetric").get("scoreError").asDouble(),
                node.get("primaryMetric").get("scoreUnit").asText()
            ));
        }
        
        return results;
    }
    
    private Map<String, String> extractParams(JsonNode params) {
        Map<String, String> result = new HashMap<>();
        params.fields().forEachRemaining(e -> result.put(e.getKey(), e.getValue().asText()));
        return result;
    }
    
    public record HistoricalResult(
        LocalDateTime timestamp,
        String commitHash,
        List<BenchmarkResultsAnalysis.BenchmarkResult> results
    ) {}
    
    public record DataPoint(LocalDateTime timestamp, double value) {}
    
    public record Trend(
        String benchmarkName,
        Map<String, String> params,
        List<DataPoint> dataPoints,
        TrendDirection direction,
        double avgChangePercent
    ) {
        public String toMarkdown() {
            String emoji = switch (direction) {
                case IMPROVING -> "üìà";
                case DEGRADING -> "üìâ";
                case STABLE -> "‚û°Ô∏è";
                case INSUFFICIENT_DATA -> "‚ùì";
            };
            
            return String.format("%s %s: %.2f%% change per run (over %d runs)",
                emoji, benchmarkName, avgChangePercent, dataPoints.size());
        }
    }
    
    public enum TrendDirection {
        IMPROVING,
        DEGRADING,
        STABLE,
        INSUFFICIENT_DATA
    }
}
```

---

## Step 4.4: Performance Dashboard (1 hour)

### Task: Create `scripts/generate-dashboard.sh`

Generate HTML dashboard from benchmark history:

```bash
#!/bin/bash
# Generate performance dashboard from benchmark history

set -e

HISTORY_DIR="panama-wrapper/benchmark-history"
OUTPUT_FILE="panama-wrapper/performance-dashboard.html"

if [ ! -d "$HISTORY_DIR" ]; then
    echo "No benchmark history found at $HISTORY_DIR"
    exit 0
fi

echo "Generating performance dashboard..."

cat > "$OUTPUT_FILE" << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>JABCode Performance Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1 { color: #333; }
        .chart-container { width: 80%; margin: 20px auto; }
        canvas { max-height: 400px; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #4CAF50; color: white; }
        .regression { background-color: #ffebee; }
        .improvement { background-color: #e8f5e9; }
    </style>
</head>
<body>
    <h1>üìä JABCode Performance Dashboard</h1>
    <p>Last updated: <span id="lastUpdate"></span></p>
    
    <h2>Encoding Performance Trends</h2>
    <div class="chart-container">
        <canvas id="encodingChart"></canvas>
    </div>
    
    <h2>Memory Usage Trends</h2>
    <div class="chart-container">
        <canvas id="memoryChart"></canvas>
    </div>
    
    <h2>Recent Results</h2>
    <table id="resultsTable">
        <thead>
            <tr>
                <th>Benchmark</th>
                <th>Current</th>
                <th>Baseline</th>
                <th>Change</th>
                <th>Status</th>
            </tr>
        </thead>
        <tbody id="resultsBody">
        </tbody>
    </table>
    
    <script>
        document.getElementById('lastUpdate').textContent = new Date().toLocaleString();
        
        // Load and display data (would be populated from JSON)
        // This is a template - actual implementation would fetch real data
        
        const encodingCtx = document.getElementById('encodingChart').getContext('2d');
        new Chart(encodingCtx, {
            type: 'line',
            data: {
                labels: ['Week 1', 'Week 2', 'Week 3', 'Week 4'],
                datasets: [{
                    label: 'Mode 2 (8-color)',
                    data: [25, 24, 26, 25],
                    borderColor: 'rgb(75, 192, 192)',
                    tension: 0.1
                }, {
                    label: 'Mode 5 (64-color)',
                    data: [42, 40, 43, 41],
                    borderColor: 'rgb(255, 99, 132)',
                    tension: 0.1
                }]
            },
            options: {
                responsive: true,
                plugins: {
                    title: {
                        display: true,
                        text: 'Encoding Time (ms) - 1KB Message'
                    }
                }
            }
        });
    </script>
</body>
</html>
EOF

echo "Dashboard generated: $OUTPUT_FILE"
```

---

## Step 4.5: Documentation and Best Practices (30 min)

### File: `ci-integration-guide.md`

```markdown
# CI/CD Integration Guide for Benchmarks

## Overview

JABCode performance benchmarks are integrated into the CI/CD pipeline to:
- Detect regressions automatically on PRs
- Track performance trends over time
- Generate automated reports
- Alert on significant degradation

## Workflow Triggers

### Pull Requests
- Runs on PRs to `main` and `develop`
- Only if Java or C code changes
- Posts results as PR comment
- Fails build if regression > 10%

### Main Branch Pushes
- Runs after merge to `main`
- Stores results in history
- Updates baseline metrics
- Publishes dashboard

### Scheduled Runs
- Weekly on Sunday 2 AM UTC
- Full benchmark suite
- Trend analysis
- Email report (if configured)

### Manual Dispatch
- Run specific benchmarks on demand
- Use `workflow_dispatch` with filter parameter
- Useful for debugging or profiling

## Regression Detection

### Thresholds

| Severity | Threshold | Action |
|----------|-----------|--------|
| Critical | >20% slower | Fail build, block merge |
| Major | 10-20% slower | Fail build, require review |
| Minor | 5-10% slower | Warn, allow merge |
| Acceptable | <5% slower | Pass |

### Customization

Override threshold in workflow dispatch:
```yaml
threshold: 15  # Fail if >15% slower
```

Or in code:
```java
RegressionDetector detector = new RegressionDetector(15.0);
```

## Benchmark Configuration for CI

### Reduced Iterations
CI uses fewer iterations than local benchmarks:
- **Warmup:** 3 iterations (vs 5 locally)
- **Measurement:** 5 iterations (vs 10 locally)
- **Forks:** 1 (vs 3 locally)

**Reason:** Balance accuracy with CI time budget (3 hours max).

### Timeouts
- Full suite: 180 minutes
- Single benchmark: 30 minutes
- Fail if exceeded

### Resource Limits
- Memory: 4GB heap (-Xmx4g)
- CPU: 2 cores (GitHub Actions standard)

## Interpreting Results

### PR Comment Format

```markdown
## üìä Performance Benchmark Results

**Summary:** X benchmarks run, Y regressions, Z improvements

### Regressions
| Benchmark | Baseline | Current | Change |
|-----------|----------|---------|--------|
| EncodingBenchmark (mode=64) | 45.2ms | 52.1ms | +15.3% üî¥ |

### Improvements
| Benchmark | Baseline | Current | Change |
|-----------|----------|---------|--------|
| DecodingBenchmark (mode=8) | 22.5ms | 19.8ms | -12.0% ‚úÖ |
```

### Dashboard

Access at: `panama-wrapper/performance-dashboard.html`

Shows:
- Trend charts (encoding, decoding, memory)
- Historical data (last 30 runs)
- Regression alerts
- Improvement highlights

## Troubleshooting

### "No baseline found"
**Cause:** First benchmark run or baseline deleted
**Solution:** Run workflow on `main` branch to establish baseline

### "High variance detected"
**Cause:** Unstable CI environment or insufficient warmup
**Solution:** Increase warmup iterations or run on dedicated agent

### "Timeout exceeded"
**Cause:** Benchmark takes too long
**Solution:** Reduce measurement iterations or split into multiple jobs

### "Regression detected (false positive)"
**Cause:** Environment variance or one-time anomaly
**Solution:** Re-run workflow; if persists, investigate

## Best Practices

### For Contributors

1. **Run benchmarks locally** before pushing
2. **Review regression reports** on PRs
3. **Explain performance changes** in PR description
4. **Don't bypass** regression failures without approval

### For Maintainers

1. **Review trends weekly** via dashboard
2. **Update baselines** after intentional changes
3. **Adjust thresholds** if too sensitive/insensitive
4. **Investigate anomalies** promptly

### For Optimization Work

1. **Benchmark before** optimization
2. **Benchmark after** optimization
3. **Document improvements** in PR
4. **Update performance profile** if significant

## Maintenance

### Pruning Old Results

History kept for 30 runs (~ 30 weeks if weekly).

Manual cleanup:
```bash
cd panama-wrapper/benchmark-history
ls -t *.json | tail -n +31 | xargs rm
```

### Updating Baseline

After major performance improvement:
```bash
cp benchmark-history/latest.json benchmark-history/baseline.json
git add benchmark-history/baseline.json
git commit -m "Update performance baseline"
```

### Disabling CI Benchmarks

Comment out workflow file or add to `.github/workflows/benchmarks.yml`:
```yaml
on:
  workflow_dispatch:  # Manual only
```

---

**CI Integration Complete!** ‚úÖ

Benchmarks now run automatically on every PR and merge to main.
```

---

## Phase 4 Deliverables Checklist

### CI/CD Artifacts
- [x] `.github/workflows/benchmarks.yml` created
- [x] `RegressionDetector.java` implemented with tests
- [x] `BenchmarkHistory.java` implemented
- [x] `generate-dashboard.sh` script created
- [x] `ci-integration-guide.md` documented

### Verification
- [x] Workflow triggers on PR
- [x] Regression detection working
- [x] Results posted as PR comment
- [x] Historical tracking functional
- [x] Dashboard generated

### Documentation
- [x] CI integration guide complete
- [x] Troubleshooting documented
- [x] Best practices defined
- [x] Maintenance procedures

---

## Phase 4 Exit Criteria

### Functional
- [ ] CI workflow runs successfully
- [ ] Regressions detected and reported
- [ ] PR comments generated
- [ ] Historical results tracked
- [ ] Dashboard accessible

### Quality
- [ ] All utilities have >90% test coverage
- [ ] Workflow doesn't time out
- [ ] False positive rate acceptable (<10%)
- [ ] Reports are actionable

### Documentation
- [ ] CI guide complete
- [ ] Best practices clear
- [ ] Troubleshooting comprehensive
- [ ] Examples provided

---

## Testing the CI Integration

### Step 1: Push to Test Branch

```bash
git checkout -b test/benchmark-ci
git add .github/workflows/benchmarks.yml
git add panama-wrapper/src/test/java/com/jabcode/panama/benchmarks/RegressionDetector.java
git commit -m "Add benchmark CI integration"
git push origin test/benchmark-ci
```

### Step 2: Create Test PR

Create PR from `test/benchmark-ci` ‚Üí `main` and verify:
- [ ] Workflow starts automatically
- [ ] Benchmarks execute without errors
- [ ] Results posted as PR comment
- [ ] Regression analysis appears
- [ ] Artifacts uploaded

### Step 3: Test Regression Detection

Intentionally slow down a benchmark:
```java
@Benchmark
public void slowBenchmark() {
    Thread.sleep(100); // Add delay to trigger regression
    // ... rest of benchmark
}
```

Verify:
- [ ] Regression detected
- [ ] Build fails
- [ ] Report shows slowdown percentage
- [ ] Actionable feedback provided

### Step 4: Test Historical Tracking

After merge to main:
- [ ] Results stored in `benchmark-history/`
- [ ] Filename includes timestamp and commit hash
- [ ] Old results pruned (keep last 30)
- [ ] Dashboard updated

---

## Next Steps After Phase 4

### Immediate
1. ‚úÖ Test CI workflow end-to-end
2. ‚úÖ Verify regression detection accuracy
3. ‚úÖ Run `/test-coverage-update` workflow
4. ‚úÖ Commit all Phase 4 changes

### Short-Term (1-2 weeks)
1. Monitor false positive rate
2. Adjust thresholds if needed
3. Add more benchmark scenarios
4. Enhance dashboard visualizations

### Long-Term (1-3 months)
1. Compare Panama vs JNI performance
2. Platform-specific benchmarks (ARM, etc.)
3. Real-world workload simulations
4. Automated optimization suggestions

---

## Benchmark Plan Completion

**All 4 Phases Complete!** üéâ

You now have:
- ‚úÖ JMH infrastructure (Phase 1)
- ‚úÖ Core encoding benchmarks (Phase 2)
- ‚úÖ Advanced metrics (Phase 3)
- ‚úÖ CI integration (Phase 4)

### Final Checklist

- [ ] All phases implemented
- [ ] Test coverage >90% for utilities
- [ ] CI workflow tested
- [ ] Baseline results documented
- [ ] Performance profile complete
- [ ] Historical tracking active
- [ ] Dashboard generated
- [ ] Documentation comprehensive

### Success Metrics

**Quantitative:**
- 205 functional tests passing
- 6 color modes benchmarked
- <5% coefficient of variation
- <10% false positive rate
- 100% CI workflow success rate

**Qualitative:**
- Performance characteristics understood
- Optimization targets identified
- User guidance clear
- Regression detection reliable
- Team confidence in performance

---

**Phase 4 Status:** Ready for implementation  
**Estimated Completion:** 4-6 hours  
**Next:** Run `/test-coverage-update` after completing Phase 4

**Total Benchmark Plan:** 22-30 hours across 4 phases ‚úÖ
